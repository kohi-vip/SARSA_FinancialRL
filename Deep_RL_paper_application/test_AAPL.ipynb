{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3f8fcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import các thư viện cần thiết\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1a541c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep RL libraries imported successfully!\n",
      "PyTorch version: 2.7.0+cpu\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "# Import thêm các thư viện cho Deep RL\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "import random\n",
    "from copy import deepcopy\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"Deep RL libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ed4253b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đang tải dữ liệu training (Good situation)...\n",
      "YF.download() has changed argument auto_adjust default to True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  6 of 6 completed\n"
     ]
    }
   ],
   "source": [
    "# Tải dữ liệu training cho Good situation\n",
    "print(\"Đang tải dữ liệu training (Good situation)...\")\n",
    "df_train_good = yf.download(tickers, start=train_good_start, end=train_good_end, group_by='ticker')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c84834d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đang tải dữ liệu test (Good situation)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  6 of 6 completed\n"
     ]
    }
   ],
   "source": [
    "# Tải dữ liệu test cho Good situation\n",
    "print(\"Đang tải dữ liệu test (Good situation)...\")\n",
    "df_test_good = yf.download(tickers, start=test_good_start, end=test_good_end, group_by='ticker')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c3f93af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đang tải dữ liệu training (Bad situation)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  6 of 6 completed\n"
     ]
    }
   ],
   "source": [
    "# Tải dữ liệu training cho Bad situation\n",
    "print(\"Đang tải dữ liệu training (Bad situation)...\")\n",
    "df_train_bad = yf.download(tickers, start=train_bad_start, end=train_bad_end, group_by='ticker')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8cf0d517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đang tải dữ liệu test (Bad situation)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  6 of 6 completed\n"
     ]
    }
   ],
   "source": [
    "# Tải dữ liệu test cho Bad situation\n",
    "print(\"Đang tải dữ liệu test (Bad situation)...\")\n",
    "df_test_bad = yf.download(tickers, start=test_bad_start, end=test_bad_end, group_by='ticker')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c02542",
   "metadata": {},
   "source": [
    "### Gắn dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dba51519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Technical indicators calculated!\n",
      "\n",
      "Columns: ['Open', 'High', 'Low', 'Close', 'Volume', 'MACD', 'RSI', 'CCI', 'ADX']\n",
      "\n",
      "Sample data:\n",
      "Price           Close      MACD        RSI         CCI        ADX\n",
      "Date                                                             \n",
      "2018-12-24  34.936462 -2.477554  11.321534 -143.637251  42.996215\n",
      "2018-12-26  37.396755 -2.386403  30.982132  -87.823527  45.199311\n",
      "2018-12-27  37.154049 -2.307154  31.566454  -71.825673  46.995164\n",
      "2018-12-28  37.173096 -2.217253  36.137583  -45.891443  47.928414\n",
      "2018-12-31  37.532394 -2.092887  36.710180  -27.391718  47.097537\n"
     ]
    }
   ],
   "source": [
    "# Calculate Technical Indicators (MACD, RSI, CCI, ADX)\n",
    "def calculate_technical_indicators(df, window=14):\n",
    "    \"\"\"\n",
    "    Calculate technical indicators for a single stock dataframe\n",
    "    \"\"\"\n",
    "    data = df.copy()\n",
    "    \n",
    "    # MACD (Moving Average Convergence Divergence)\n",
    "    exp1 = data['Close'].ewm(span=12, adjust=False).mean()\n",
    "    exp2 = data['Close'].ewm(span=26, adjust=False).mean()\n",
    "    data['MACD'] = exp1 - exp2\n",
    "    \n",
    "    # RSI (Relative Strength Index)\n",
    "    delta = data['Close'].diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
    "    rs = gain / loss\n",
    "    data['RSI'] = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    # CCI (Commodity Channel Index)\n",
    "    tp = (data['High'] + data['Low'] + data['Close']) / 3\n",
    "    data['CCI'] = (tp - tp.rolling(window=window).mean()) / (0.015 * tp.rolling(window=window).std())\n",
    "    \n",
    "    # ADX (Average Directional Index) - Simplified version\n",
    "    high_diff = data['High'].diff()\n",
    "    low_diff = -data['Low'].diff()\n",
    "    \n",
    "    pos_dm = high_diff.where((high_diff > low_diff) & (high_diff > 0), 0)\n",
    "    neg_dm = low_diff.where((low_diff > high_diff) & (low_diff > 0), 0)\n",
    "    \n",
    "    tr = pd.concat([data['High'] - data['Low'], \n",
    "                    (data['High'] - data['Close'].shift()).abs(), \n",
    "                    (data['Low'] - data['Close'].shift()).abs()], axis=1).max(axis=1)\n",
    "    \n",
    "    atr = tr.rolling(window=window).mean()\n",
    "    pos_di = 100 * (pos_dm.rolling(window=window).mean() / atr)\n",
    "    neg_di = 100 * (neg_dm.rolling(window=window).mean() / atr)\n",
    "    \n",
    "    dx = 100 * (pos_di - neg_di).abs() / (pos_di + neg_di)\n",
    "    data['ADX'] = dx.rolling(window=window).mean()\n",
    "    \n",
    "    # Fill NaN values\n",
    "    data.fillna(method='bfill', inplace=True)\n",
    "    data.fillna(method='ffill', inplace=True)\n",
    "    data.fillna(0, inplace=True)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Test with one stock\n",
    "test_data = calculate_technical_indicators(df_train_good['AAPL'])\n",
    "print(\"Technical indicators calculated!\")\n",
    "print(f\"\\nColumns: {test_data.columns.tolist()}\")\n",
    "print(f\"\\nSample data:\")\n",
    "print(test_data[['Close', 'MACD', 'RSI', 'CCI', 'ADX']].tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b836eb88",
   "metadata": {},
   "source": [
    "### Xây dựng môi trường và agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17ee60b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Stock Trading Environment...\n",
      "\n",
      "Initial state shape: (7,)\n",
      "Initial state: [ 0.01590992  1.          0.          0.          0.2862478  -0.59649104\n",
      "  0.4053651 ]\n",
      "Action space size: 11\n",
      "Action space: [-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "class StockTradingEnv:\n",
    "    \"\"\"\n",
    "    Stock Trading Environment following Yang et al. (2020)\n",
    "    \n",
    "    State: [price, balance, holdings, MACD, RSI, CCI, ADX]\n",
    "    Action: {-k, ..., 0, ..., k} where negative=sell, positive=buy, 0=hold\n",
    "    Reward: Change in portfolio value - trading fees\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data, initial_balance=1000, max_shares=5, fee_rate=0.001, min_balance_tolerance=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data: DataFrame with columns ['Close', 'MACD', 'RSI', 'CCI', 'ADX']\n",
    "            initial_balance: Starting cash balance\n",
    "            max_shares: Maximum number of shares to trade in one action (k)\n",
    "            fee_rate: Trading fee rate (default 0.1% = 0.001)\n",
    "            min_balance_tolerance: Minimum balance allowed (e.g., 0 or -100)\n",
    "        \"\"\"\n",
    "        self.data = data.reset_index(drop=True)\n",
    "        self.initial_balance = initial_balance\n",
    "        self.max_shares = max_shares\n",
    "        self.fee_rate = fee_rate\n",
    "        self.min_balance_tolerance = min_balance_tolerance\n",
    "        \n",
    "        # Action space: {-k, -k+1, ..., -1, 0, 1, ..., k-1, k}\n",
    "        self.action_space = list(range(-max_shares, max_shares + 1))\n",
    "        self.n_actions = len(self.action_space)\n",
    "        \n",
    "        # State dimension: [price, balance, holdings, MACD, RSI, CCI, ADX]\n",
    "        self.state_dim = 7\n",
    "        \n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset environment to initial state\"\"\"\n",
    "        self.current_step = 0\n",
    "        self.balance = self.initial_balance\n",
    "        self.holdings = 0\n",
    "        self.total_trades = 0\n",
    "        self.trade_history = []\n",
    "        \n",
    "        return self._get_state()\n",
    "    \n",
    "    def _get_state(self):\n",
    "        \"\"\"Get current state representation\"\"\"\n",
    "        row = self.data.iloc[self.current_step]\n",
    "        \n",
    "        state = np.array([\n",
    "            row['Close'] / 1000.0,  # Normalize price\n",
    "            self.balance / self.initial_balance,  # Normalize balance\n",
    "            self.holdings / self.max_shares if self.max_shares > 0 else 0,  # Normalize holdings\n",
    "            row['MACD'] / 100.0,  # Normalize MACD\n",
    "            row['RSI'] / 100.0,  # Normalize RSI (0-100)\n",
    "            row['CCI'] / 200.0,  # Normalize CCI\n",
    "            row['ADX'] / 100.0   # Normalize ADX (0-100)\n",
    "        ], dtype=np.float32)\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def step(self, action_idx):\n",
    "        \"\"\"\n",
    "        Execute one step in the environment\n",
    "        \n",
    "        Args:\n",
    "            action_idx: Index of action in action_space\n",
    "            \n",
    "        Returns:\n",
    "            next_state, reward, done, info\n",
    "        \"\"\"\n",
    "        action = self.action_space[action_idx]  # Convert index to actual action\n",
    "        \n",
    "        current_price = self.data.iloc[self.current_step]['Close']\n",
    "        prev_portfolio_value = self.balance + current_price * self.holdings\n",
    "        \n",
    "        # Execute action with constraints\n",
    "        executed_shares = self._execute_action(action, current_price)\n",
    "        \n",
    "        # Move to next step\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= len(self.data) - 1\n",
    "        \n",
    "        if not done:\n",
    "            next_price = self.data.iloc[self.current_step]['Close']\n",
    "        else:\n",
    "            next_price = current_price\n",
    "        \n",
    "        # Calculate reward: change in portfolio value\n",
    "        new_portfolio_value = self.balance + next_price * self.holdings\n",
    "        reward = new_portfolio_value - prev_portfolio_value\n",
    "        \n",
    "        next_state = self._get_state() if not done else None\n",
    "        \n",
    "        info = {\n",
    "            'portfolio_value': new_portfolio_value,\n",
    "            'balance': self.balance,\n",
    "            'holdings': self.holdings,\n",
    "            'executed_shares': executed_shares,\n",
    "            'price': next_price\n",
    "        }\n",
    "        \n",
    "        return next_state, reward, done, info\n",
    "    \n",
    "    def _execute_action(self, action, current_price):\n",
    "        \"\"\"\n",
    "        Execute trading action with constraints\n",
    "        \n",
    "        Args:\n",
    "            action: Number of shares to trade (negative=sell, positive=buy, 0=hold)\n",
    "            current_price: Current stock price\n",
    "            \n",
    "        Returns:\n",
    "            executed_shares: Actual number of shares traded\n",
    "        \"\"\"\n",
    "        executed_shares = 0\n",
    "        \n",
    "        if action > 0:  # Buy\n",
    "            # Constraint 1: Cannot make balance below min_balance_tolerance\n",
    "            max_affordable = int((self.balance - self.min_balance_tolerance) / current_price)\n",
    "            shares_to_buy = min(action, max_affordable)\n",
    "            \n",
    "            if shares_to_buy > 0:\n",
    "                cost = shares_to_buy * current_price\n",
    "                fee = self.fee_rate * cost\n",
    "                total_cost = cost + fee\n",
    "                \n",
    "                if self.balance >= total_cost + self.min_balance_tolerance:\n",
    "                    self.balance -= total_cost\n",
    "                    self.holdings += shares_to_buy\n",
    "                    executed_shares = shares_to_buy\n",
    "                    self.total_trades += 1\n",
    "                    self.trade_history.append(('BUY', shares_to_buy, current_price, self.current_step))\n",
    "        \n",
    "        elif action < 0:  # Sell\n",
    "            # Constraint 2: Cannot sell more than current holdings\n",
    "            shares_to_sell = min(abs(action), self.holdings)\n",
    "            \n",
    "            if shares_to_sell > 0:\n",
    "                revenue = shares_to_sell * current_price\n",
    "                fee = self.fee_rate * revenue\n",
    "                net_revenue = revenue - fee\n",
    "                \n",
    "                self.balance += net_revenue\n",
    "                self.holdings -= shares_to_sell\n",
    "                executed_shares = -shares_to_sell\n",
    "                self.total_trades += 1\n",
    "                self.trade_history.append(('SELL', shares_to_sell, current_price, self.current_step))\n",
    "        \n",
    "        # action == 0: Hold (do nothing)\n",
    "        \n",
    "        return executed_shares\n",
    "    \n",
    "    def get_portfolio_value(self):\n",
    "        \"\"\"Get current total portfolio value\"\"\"\n",
    "        current_price = self.data.iloc[self.current_step]['Close']\n",
    "        return self.balance + current_price * self.holdings\n",
    "\n",
    "# Test the environment\n",
    "print(\"Testing Stock Trading Environment...\")\n",
    "test_env = StockTradingEnv(test_data, initial_balance=1000, max_shares=5)\n",
    "state = test_env.reset()\n",
    "print(f\"\\nInitial state shape: {state.shape}\")\n",
    "print(f\"Initial state: {state}\")\n",
    "print(f\"Action space size: {test_env.n_actions}\")\n",
    "print(f\"Action space: {test_env.action_space}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a745afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN Network Architecture:\n",
      "DQN(\n",
      "  (network): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.2, inplace=False)\n",
      "    (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.2, inplace=False)\n",
      "    (6): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Dropout(p=0.2, inplace=False)\n",
      "    (9): Linear(in_features=64, out_features=21, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Total parameters: 27,157\n"
     ]
    }
   ],
   "source": [
    "class DQN(nn.Module):\n",
    "    \"\"\"Deep Q-Network\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, n_actions, hidden_dims=[128, 128, 64]):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        input_dim = state_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(0.2))\n",
    "            input_dim = hidden_dim\n",
    "        \n",
    "        layers.append(nn.Linear(input_dim, n_actions))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Test DQN\n",
    "test_dqn = DQN(state_dim=7, n_actions=21)\n",
    "print(\"DQN Network Architecture:\")\n",
    "print(test_dqn)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in test_dqn.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48604eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replay Buffer implemented!\n"
     ]
    }
   ],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Experience Replay Buffer for DQN\"\"\"\n",
    "    \n",
    "    def __init__(self, capacity=10000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return (np.array(states), np.array(actions), np.array(rewards), \n",
    "                np.array(next_states), np.array(dones))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "print(\"Replay Buffer implemented!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab48dafb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DQN Agent with alpha implemented successfully!\n"
     ]
    }
   ],
   "source": [
    "class DQNAgent:\n",
    "    \"\"\"Deep Q-Learning Agent with alpha blending parameter\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, n_actions, lr=0.00001, gamma=0.6, alpha=0.7,\n",
    "                 epsilon_start=0.8, epsilon_end=0.2, epsilon_decay=0.9,\n",
    "                 buffer_capacity=10000, batch_size=64, target_update=10):\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.n_actions = n_actions\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha  # <--- thêm alpha\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.batch_size = batch_size\n",
    "        self.target_update = target_update\n",
    "        \n",
    "        # Q-networks\n",
    "        self.q_network = DQN(state_dim, n_actions).to(self.device)\n",
    "        self.target_network = DQN(state_dim, n_actions).to(self.device)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        self.target_network.eval()\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "        self.replay_buffer = ReplayBuffer(buffer_capacity)\n",
    "        \n",
    "        self.update_counter = 0\n",
    "        self.loss_history = []\n",
    "    \n",
    "    def select_action(self, state, training=True):\n",
    "        \"\"\"Epsilon-greedy action selection\"\"\"\n",
    "        if training and random.random() < self.epsilon:\n",
    "            return random.randrange(self.n_actions)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "                q_values = self.q_network(state_tensor)\n",
    "                return q_values.argmax(dim=1).item()\n",
    "    \n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store transition in replay buffer\"\"\"\n",
    "        self.replay_buffer.push(state, action, reward, next_state, done)\n",
    "    \n",
    "    def train_step(self):\n",
    "        \"\"\"Perform one training step\"\"\"\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return None\n",
    "        \n",
    "        # Sample batch\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        states = torch.FloatTensor(states).to(self.device)\n",
    "        actions = torch.LongTensor(actions).to(self.device)\n",
    "        rewards = torch.FloatTensor(rewards).to(self.device)\n",
    "        next_states = torch.FloatTensor(next_states).to(self.device)\n",
    "        dones = torch.FloatTensor(dones).to(self.device)\n",
    "        \n",
    "        # Current Q values\n",
    "        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        # Target Q values (with alpha blending)\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_network(next_states).max(1)[0]\n",
    "            q_target_raw = rewards + (1 - dones) * self.gamma * next_q_values\n",
    "            target_q_values = (1 - self.alpha) * current_q_values + self.alpha * q_target_raw\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = F.mse_loss(current_q_values, target_q_values)\n",
    "        \n",
    "        # Optimize\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), 1.0)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Update target network\n",
    "        self.update_counter += 1\n",
    "        if self.update_counter % self.target_update == 0:\n",
    "            self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        \n",
    "        # Decay epsilon\n",
    "        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)\n",
    "        \n",
    "        self.loss_history.append(loss.item())\n",
    "        return loss.item()\n",
    "    \n",
    "    def save(self, filepath):\n",
    "        \"\"\"Save model\"\"\"\n",
    "        torch.save({\n",
    "            'q_network': self.q_network.state_dict(),\n",
    "            'target_network': self.target_network.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "            'epsilon': self.epsilon,\n",
    "            'alpha': self.alpha\n",
    "        }, filepath)\n",
    "    \n",
    "    def load(self, filepath):\n",
    "        \"\"\"Load model\"\"\"\n",
    "        checkpoint = torch.load(filepath)\n",
    "        self.q_network.load_state_dict(checkpoint['q_network'])\n",
    "        self.target_network.load_state_dict(checkpoint['target_network'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        self.epsilon = checkpoint['epsilon']\n",
    "        self.alpha = checkpoint.get('alpha', 0.7)  \n",
    "\n",
    "print(\"✅ DQN Agent with alpha implemented successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "636acb42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep SARSA Agent implemented successfully!\n",
      "\n",
      "Key difference from DQN:\n",
      "- DQN uses max Q(s', a') for target (off-policy)\n",
      "- SARSA uses Q(s', a') where a' is actually selected action (on-policy)\n"
     ]
    }
   ],
   "source": [
    "class DeepSARSAAgent:\n",
    "    \"\"\"\n",
    "    Deep SARSA Agent\n",
    "    \n",
    "    Key difference from DQN: Uses next action selected by policy (on-policy)\n",
    "    instead of max Q-value (off-policy)\n",
    "    \n",
    "    Hyperparameters as per Yang et al. (2020):\n",
    "    - lr: 1e-5 (neural network learning rate)\n",
    "    - alpha: 0.7 (Q-function update learning rate for smooth update)\n",
    "    - gamma: 0.6 (discount factor)\n",
    "    - epsilon: 0.8 → 0.2 with decay 0.9\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, state_dim, n_actions, lr=1e-5, gamma=0.6, alpha=0.7,\n",
    "                 epsilon_start=0.8, epsilon_end=0.2, epsilon_decay=0.9,\n",
    "                 buffer_capacity=10000, batch_size=64, target_update=10):\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.n_actions = n_actions\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha  # Learning rate for Q-function smooth update\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.batch_size = batch_size\n",
    "        self.target_update = target_update\n",
    "        \n",
    "        # Q-networks\n",
    "        self.q_network = DQN(state_dim, n_actions).to(self.device)\n",
    "        self.target_network = DQN(state_dim, n_actions).to(self.device)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        self.target_network.eval()\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "        self.replay_buffer = ReplayBuffer(buffer_capacity)\n",
    "        \n",
    "        self.update_counter = 0\n",
    "        self.loss_history = []\n",
    "    \n",
    "    def select_action(self, state, training=True):\n",
    "        \"\"\"Epsilon-greedy action selection\"\"\"\n",
    "        if training and random.random() < self.epsilon:\n",
    "            return random.randrange(self.n_actions)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "                q_values = self.q_network(state_tensor)\n",
    "                return q_values.argmax(dim=1).item()\n",
    "    \n",
    "    def store_transition(self, state, action, reward, next_state, next_action, done, buffer_capacity=10000):\n",
    "        \"\"\"Store SARSA transition (includes next_action)\"\"\"\n",
    "        self.replay_buffer.push(state, action, reward, next_state, done)\n",
    "        # Store next_action separately for SARSA update\n",
    "        if not hasattr(self, 'next_actions'):\n",
    "            self.next_actions = deque(maxlen=buffer_capacity)\n",
    "        self.next_actions.append(next_action)\n",
    "    \n",
    "    def train_step(self):\n",
    "        \"\"\"Perform one SARSA training step\"\"\"\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return None\n",
    "        \n",
    "        # Sample batch\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)\n",
    "        \n",
    "        # Get corresponding next_actions\n",
    "        next_actions_batch = random.sample(list(self.next_actions), self.batch_size)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        states = torch.FloatTensor(states).to(self.device)\n",
    "        actions = torch.LongTensor(actions).to(self.device)\n",
    "        rewards = torch.FloatTensor(rewards).to(self.device)\n",
    "        next_states = torch.FloatTensor(next_states).to(self.device)\n",
    "        next_actions = torch.LongTensor(next_actions_batch).to(self.device)\n",
    "        dones = torch.FloatTensor(dones).to(self.device)\n",
    "        \n",
    "        # Current Q values: Q(s, a)\n",
    "        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        # SARSA Target with smooth update (as per paper):\n",
    "        # Q_target = (1 - α) * Q_current + α * [r + γ * Q(s', a')]\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_network(next_states).gather(1, next_actions.unsqueeze(1)).squeeze(1)\n",
    "            td_target = rewards + (1 - dones) * self.gamma * next_q_values\n",
    "            # Smooth update: blend current Q-value with TD target\n",
    "            target_q_values = (1 - self.alpha) * current_q_values + self.alpha * td_target\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = F.mse_loss(current_q_values, target_q_values)\n",
    "        \n",
    "        # Optimize\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), 1.0)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Update target network\n",
    "        self.update_counter += 1\n",
    "        if self.update_counter % self.target_update == 0:\n",
    "            self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        \n",
    "        # Decay epsilon\n",
    "        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)\n",
    "        \n",
    "        self.loss_history.append(loss.item())\n",
    "        return loss.item()\n",
    "    \n",
    "    def save(self, filepath):\n",
    "        \"\"\"Save model\"\"\"\n",
    "        torch.save({\n",
    "            'q_network': self.q_network.state_dict(),\n",
    "            'target_network': self.target_network.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "            'epsilon': self.epsilon\n",
    "        }, filepath)\n",
    "    \n",
    "    def load(self, filepath):\n",
    "        \"\"\"Load model\"\"\"\n",
    "        checkpoint = torch.load(filepath)\n",
    "        self.q_network.load_state_dict(checkpoint['q_network'])\n",
    "        self.target_network.load_state_dict(checkpoint['target_network'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        self.epsilon = checkpoint['epsilon']\n",
    "\n",
    "print(\"Deep SARSA Agent implemented successfully!\")\n",
    "print(\"\\nKey difference from DQN:\")\n",
    "print(\"- DQN uses max Q(s', a') for target (off-policy)\")\n",
    "print(\"- SARSA uses Q(s', a') where a' is actually selected action (on-policy)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9ac8344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Network implemented!\n",
      "\n",
      "Network outputs probability distribution over actions using Softmax\n"
     ]
    }
   ],
   "source": [
    "# Policy Gradient Network\n",
    "class PolicyNetwork(nn.Module):\n",
    "    \"\"\"Policy Network for Policy Gradient Method\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, n_actions, hidden_dims=[128, 128, 64]):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        input_dim = state_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(0.2))\n",
    "            input_dim = hidden_dim\n",
    "        \n",
    "        layers.append(nn.Linear(input_dim, n_actions))\n",
    "        layers.append(nn.Softmax(dim=-1))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "print(\"Policy Network implemented!\")\n",
    "print(\"\\nNetwork outputs probability distribution over actions using Softmax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e48e5d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN Training function ready!\n"
     ]
    }
   ],
   "source": [
    "def train_dqn(agent, env, n_episodes=100, print_every=10):\n",
    "    \"\"\"\n",
    "    Train DQN agent\n",
    "    \n",
    "    Args:\n",
    "        agent: DQNAgent instance\n",
    "        env: StockTradingEnv instance\n",
    "        n_episodes: Number of training episodes\n",
    "        print_every: Print statistics every N episodes\n",
    "        \n",
    "    Returns:\n",
    "        episode_rewards, episode_portfolio_values\n",
    "    \"\"\"\n",
    "    episode_rewards = []\n",
    "    episode_portfolio_values = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # Select action\n",
    "            action = agent.select_action(state, training=True)\n",
    "            \n",
    "            # Take action\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            # Store transition\n",
    "            if next_state is not None:\n",
    "                agent.store_transition(state, action, reward, next_state, done)\n",
    "            \n",
    "            # Train agent\n",
    "            loss = agent.train_step()\n",
    "            \n",
    "            episode_reward += reward\n",
    "            state = next_state if next_state is not None else state\n",
    "        \n",
    "        # Record metrics\n",
    "        final_portfolio_value = env.get_portfolio_value()\n",
    "        episode_rewards.append(episode_reward)\n",
    "        episode_portfolio_values.append(final_portfolio_value)\n",
    "        \n",
    "        # Print progress\n",
    "        if (episode + 1) % print_every == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-print_every:])\n",
    "            avg_portfolio = np.mean(episode_portfolio_values[-print_every:])\n",
    "            print(f\"Episode {episode + 1}/{n_episodes} | \"\n",
    "                  f\"Avg Reward: {avg_reward:.2f} | \"\n",
    "                  f\"Avg Portfolio: ${avg_portfolio:.2f} | \"\n",
    "                  f\"Epsilon: {agent.epsilon:.4f}\")\n",
    "    \n",
    "    return episode_rewards, episode_portfolio_values\n",
    "\n",
    "print(\"DQN Training function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a296623e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sarsa(agent, env, n_episodes=100, print_every=10):\n",
    "    \"\"\"\n",
    "    Train Deep SARSA agent\n",
    "    \n",
    "    Args:\n",
    "        agent: DeepSARSAAgent instance\n",
    "        env: StockTradingEnv instance\n",
    "        n_episodes: Number of training episodes\n",
    "        print_every: Print statistics every N episodes\n",
    "        \n",
    "    Returns:\n",
    "        episode_rewards, episode_portfolio_values\n",
    "    \"\"\"\n",
    "    episode_rewards = []\n",
    "    episode_portfolio_values = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        action = agent.select_action(state, training=True)\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # Take action\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            # Select next action (this is key for SARSA)\n",
    "            if next_state is not None:\n",
    "                next_action = agent.select_action(next_state, training=True)\n",
    "                \n",
    "                # Store SARSA transition (s, a, r, s', a')\n",
    "                agent.store_transition(state, action, reward, next_state, next_action, done)\n",
    "                \n",
    "                # Train agent\n",
    "                loss = agent.train_step()\n",
    "                \n",
    "                # Update for next iteration\n",
    "                state = next_state\n",
    "                action = next_action\n",
    "            else:\n",
    "                done = True\n",
    "            \n",
    "            episode_reward += reward\n",
    "        \n",
    "        # Record metrics\n",
    "        final_portfolio_value = env.get_portfolio_value()\n",
    "        episode_rewards.append(episode_reward)\n",
    "        episode_portfolio_values.append(final_portfolio_value)\n",
    "        \n",
    "        # Print progress\n",
    "        if (episode + 1) % print_every == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-print_every:])\n",
    "            avg_portfolio = np.mean(episode_portfolio_values[-print_every:])\n",
    "            print(f\"Episode {episode + 1}/{n_episodes} | \"\n",
    "                  f\"Avg Reward: {avg_reward:.2f} | \"\n",
    "                  f\"Avg Portfolio: ${avg_portfolio:.2f} | \"\n",
    "                  f\"Epsilon: {agent.epsilon:.4f}\")\n",
    "    \n",
    "    return episode_rewards, episode_portfolio_values\n",
    "\n",
    "print(\"SARSA Training function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a19e87a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Gradient training function ready!\n"
     ]
    }
   ],
   "source": [
    "def train_policy_gradient(agent, env, n_episodes=30, print_every=10):\n",
    "    \"\"\"\n",
    "    Train Policy Gradient agent\n",
    "    \n",
    "    Args:\n",
    "        agent: PolicyGradientAgent instance\n",
    "        env: StockTradingEnv instance\n",
    "        n_episodes: Number of training episodes (paper uses 30)\n",
    "        print_every: Print statistics every N episodes\n",
    "        \n",
    "    Returns:\n",
    "        episode_rewards, episode_portfolio_values\n",
    "    \"\"\"\n",
    "    episode_rewards = []\n",
    "    episode_portfolio_values = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        # Collect episode trajectory\n",
    "        while not done:\n",
    "            # Select action from policy\n",
    "            action = agent.select_action(state, training=True)\n",
    "            \n",
    "            # Take action\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            # Store reward\n",
    "            agent.store_reward(reward)\n",
    "            \n",
    "            episode_reward += reward\n",
    "            state = next_state if next_state is not None else state\n",
    "        \n",
    "        # Train on complete episode\n",
    "        loss = agent.train_episode()\n",
    "        \n",
    "        # Record metrics\n",
    "        final_portfolio_value = env.get_portfolio_value()\n",
    "        episode_rewards.append(episode_reward)\n",
    "        episode_portfolio_values.append(final_portfolio_value)\n",
    "        \n",
    "        # Print progress\n",
    "        if (episode + 1) % print_every == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-print_every:])\n",
    "            avg_portfolio = np.mean(episode_portfolio_values[-print_every:])\n",
    "            print(f\"Episode {episode + 1}/{n_episodes} | \"\n",
    "                  f\"Avg Reward: {avg_reward:.2f} | \"\n",
    "                  f\"Avg Portfolio: ${avg_portfolio:.2f}\")\n",
    "    \n",
    "    return episode_rewards, episode_portfolio_values\n",
    "\n",
    "print(\"Policy Gradient training function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87a4d8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annual return calculation function ready!\n"
     ]
    }
   ],
   "source": [
    "def calculate_annual_return(env, agent, agent_name=\"Agent\"):\n",
    "    \"\"\"\n",
    "    Calculate annual return percentage on test set\n",
    "    \n",
    "    Args:\n",
    "        env: Test environment\n",
    "        agent: Trained agent\n",
    "        agent_name: Name for printing\n",
    "        \n",
    "    Returns:\n",
    "        annual_return_pct: Annualized return percentage\n",
    "    \"\"\"\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    initial_value = env.get_portfolio_value()\n",
    "    \n",
    "    while not done:\n",
    "        action = agent.select_action(state, training=False)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        state = next_state if next_state is not None else state\n",
    "    \n",
    "    final_value = env.get_portfolio_value()\n",
    "    total_return = (final_value - initial_value) / initial_value\n",
    "    \n",
    "    # Calculate number of years in test period\n",
    "    n_days = len(env.data)\n",
    "    n_years = n_days / 252  # 252 trading days per year\n",
    "    \n",
    "    # Annualize the return\n",
    "    annual_return = ((1 + total_return) ** (1 / n_years)) - 1\n",
    "    annual_return_pct = annual_return * 100\n",
    "    \n",
    "    print(f\"\\n{agent_name} Results:\")\n",
    "    print(f\"  Initial Portfolio: ${initial_value:.2f}\")\n",
    "    print(f\"  Final Portfolio: ${final_value:.2f}\")\n",
    "    print(f\"  Total Return: {total_return*100:.2f}%\")\n",
    "    print(f\"  Test Period: {n_days} days ({n_years:.2f} years)\")\n",
    "    print(f\"  Annual Return: {annual_return_pct:.2f}%\")\n",
    "    \n",
    "    return annual_return_pct\n",
    "\n",
    "print(\"Annual return calculation function ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b959f6",
   "metadata": {},
   "source": [
    "### Training stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6e6150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for Good Period experiment\n",
    "print(\"=\"*80)\n",
    "print(\"PREPARING DATA - GOOD PERIOD (AAPL)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate technical indicators\n",
    "aapl_train_good = calculate_technical_indicators(df_train_good['AAPL'])\n",
    "aapl_test_good = calculate_technical_indicators(df_test_good['AAPL'])\n",
    "\n",
    "# Create environments with k=5 (as per paper)\n",
    "train_env_good = StockTradingEnv(\n",
    "    aapl_train_good, \n",
    "    initial_balance=1000, \n",
    "    max_shares=5,  # k=5 as per paper\n",
    "    fee_rate=0.001,  # 0.1%\n",
    "    min_balance_tolerance=0\n",
    ")\n",
    "\n",
    "test_env_good = StockTradingEnv(\n",
    "    aapl_test_good, \n",
    "    initial_balance=1000, \n",
    "    max_shares=5,\n",
    "    fee_rate=0.001,\n",
    "    min_balance_tolerance=0\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining period: {len(aapl_train_good)} days\")\n",
    "print(f\"Test period: {len(aapl_test_good)} days\")\n",
    "print(f\"\\nEnvironment configuration:\")\n",
    "print(f\"  - Initial balance: $10,000\")\n",
    "print(f\"  - Max shares per action (k): 5\")\n",
    "print(f\"  - Trading fee: 0.1%\")\n",
    "print(f\"  - State dimension: {train_env_good.state_dim}\")\n",
    "print(f\"  - Number of actions: {train_env_good.n_actions}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
