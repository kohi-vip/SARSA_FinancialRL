{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7e726d2",
   "metadata": {},
   "source": [
    "# Policy Gradient Training for Financial Trading\n",
    "\n",
    "This notebook demonstrates how to train a Policy Gradient agent for financial trading using the organized codebase structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7486ed7",
   "metadata": {},
   "source": [
    "## 1. Importing Agents\n",
    "\n",
    "Import the agent classes from the respective modules in the agents folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35786b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ All imports successful!\n",
      "PyTorch version: 2.7.0+cpu\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath('__file__'))))\n",
    "\n",
    "# Import agents\n",
    "from agents.policy_gradient.policy_gradient_agent import (\n",
    "    PolicyGradientAgent,\n",
    "    train_policy_gradient_agent,\n",
    "    evaluate_policy_gradient_agent\n",
    ")\n",
    "\n",
    "# Import environment\n",
    "from environments.env_stocktrading import StockTradingEnv\n",
    "\n",
    "print(\"âœ“ All imports successful!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfae6b4",
   "metadata": {},
   "source": [
    "## 2. DQN Agent Implementation\n",
    "\n",
    "Demonstrate the DQN agent from dqn_agent.py, including initialization and training logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c34c203",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "DQNAgent.__init__() got an unexpected keyword argument 'action_dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m state_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m  \u001b[38;5;66;03m# Example state dimension\u001b[39;00m\n\u001b[0;32m      6\u001b[0m action_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m  \u001b[38;5;66;03m# Example action dimension (hold, buy, sell)\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m dqn_agent \u001b[38;5;241m=\u001b[39m \u001b[43mDQNAgent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43maction_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.99\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepsilon_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepsilon_end\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepsilon_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.995\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_update_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\n\u001b[0;32m     20\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ“ DQN Agent initialized\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mState dim: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdqn_agent\u001b[38;5;241m.\u001b[39mstate_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: DQNAgent.__init__() got an unexpected keyword argument 'action_dim'"
     ]
    }
   ],
   "source": [
    "# Import DQN agent for comparison\n",
    "from agents.dqn.dqn_agent import DQNAgent\n",
    "\n",
    "# Example DQN agent initialization\n",
    "state_dim = 10  # Example state dimension\n",
    "action_dim = 3  # Example action dimension (hold, buy, sell)\n",
    "\n",
    "dqn_agent = DQNAgent(\n",
    "    state_dim=state_dim,\n",
    "    action_dim=action_dim,\n",
    "    hidden_dim=128,\n",
    "    lr=1e-3,\n",
    "    gamma=0.99,\n",
    "    epsilon_start=1.0,\n",
    "    epsilon_end=0.01,\n",
    "    epsilon_decay=0.995,\n",
    "    buffer_size=10000,\n",
    "    batch_size=64,\n",
    "    target_update_freq=100\n",
    ")\n",
    "\n",
    "print(\"âœ“ DQN Agent initialized\")\n",
    "print(f\"State dim: {dqn_agent.state_dim}\")\n",
    "print(f\"Action dim: {dqn_agent.action_dim}\")\n",
    "print(f\"Hidden dim: {dqn_agent.hidden_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b1e831",
   "metadata": {},
   "source": [
    "## 3. Policy Gradient Agent Implementation\n",
    "\n",
    "Demonstrate the Policy Gradient agent from policy_gradient_agent.py, including policy updates and gradient computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8ef3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy Gradient Agent initialization\n",
    "pg_agent = PolicyGradientAgent(\n",
    "    state_dim=state_dim,\n",
    "    action_dim=action_dim,\n",
    "    hidden_dim=128,\n",
    "    lr=1e-3,\n",
    "    gamma=0.99,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    ")\n",
    "\n",
    "print(\"âœ“ Policy Gradient Agent initialized\")\n",
    "print(f\"State dim: {pg_agent.state_dim}\")\n",
    "print(f\"Action dim: {pg_agent.action_dim}\")\n",
    "print(f\"Device: {pg_agent.device}\")\n",
    "\n",
    "# Test action selection\n",
    "test_state = np.random.randn(state_dim)\n",
    "action, log_prob = pg_agent.select_action(test_state)\n",
    "print(f\"Test state shape: {test_state.shape}\")\n",
    "print(f\"Selected action: {action}\")\n",
    "print(f\"Log probability: {log_prob:.4f}\")\n",
    "\n",
    "# Test policy network\n",
    "state_tensor = torch.FloatTensor(test_state).unsqueeze(0).to(pg_agent.device)\n",
    "with torch.no_grad():\n",
    "    probs = pg_agent.policy_net(state_tensor)\n",
    "    print(f\"Action probabilities: {probs.cpu().numpy().flatten()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4334a4e1",
   "metadata": {},
   "source": [
    "## 4. Deep SARSA Agent Implementation\n",
    "\n",
    "Demonstrate the Deep SARSA agents from deep_sarsa_agent.py and deep_sarsa_agent_paper.py, including Q-value updates and action selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22536386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SARSA agents for comparison\n",
    "from agents.sarsa.deep_sarsa_agent import DeepSARSAAgent\n",
    "from agents.sarsa.deep_sarsa_agent_paper import DeepSARSAAgent as DeepSARSAAgentPaper\n",
    "\n",
    "# Mock environment for demonstration\n",
    "class MockEnv:\n",
    "    def __init__(self, state_dim=10, action_dim=3):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "    def reset(self):\n",
    "        return np.random.randn(self.state_dim)\n",
    "\n",
    "    def step(self, action):\n",
    "        next_state = np.random.randn(self.state_dim)\n",
    "        reward = np.random.randn()\n",
    "        done = np.random.rand() > 0.95  # 5% chance of episode end\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "# Initialize mock environment\n",
    "mock_env = MockEnv(state_dim, action_dim)\n",
    "\n",
    "# Initialize SARSA agents\n",
    "sarsa_agent = DeepSARSAAgent(\n",
    "    env=mock_env,\n",
    "    state_dim=state_dim,\n",
    "    action_dim=action_dim,\n",
    "    hidden_dim=128,\n",
    "    lr=1e-3,\n",
    "    gamma=0.99\n",
    ")\n",
    "\n",
    "sarsa_paper_agent = DeepSARSAAgentPaper(\n",
    "    state_dim=state_dim,\n",
    "    action_dim=action_dim,\n",
    "    hidden_dim=128,\n",
    "    lr=1e-3,\n",
    "    gamma=0.99\n",
    ")\n",
    "\n",
    "print(\"âœ“ SARSA Agents initialized\")\n",
    "print(f\"DeepSARSA Agent - State dim: {sarsa_agent.state_dim}, Action dim: {sarsa_agent.action_dim}\")\n",
    "print(f\"DeepSARSA Paper Agent - State dim: {sarsa_paper_agent.state_dim}, Action dim: {sarsa_paper_agent.action_dim}\")\n",
    "\n",
    "# Test action selection for SARSA agents\n",
    "test_state = np.random.randn(state_dim)\n",
    "action_sarsa = sarsa_agent.select_action(test_state)\n",
    "action_sarsa_paper = sarsa_paper_agent.select_action(test_state)\n",
    "\n",
    "print(f\"SARSA action: {action_sarsa}\")\n",
    "print(f\"SARSA Paper action: {action_sarsa_paper}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1af84ff",
   "metadata": {},
   "source": [
    "## 5. Training and Evaluation\n",
    "\n",
    "Demonstrate training the Policy Gradient agent and evaluating its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4733ab75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training demonstration (short training for demo)\n",
    "print(\"ðŸš€ Training Policy Gradient Agent...\")\n",
    "print(\"(This is a short demo training - increase num_episodes for real training)\")\n",
    "\n",
    "training_rewards = train_policy_gradient_agent(\n",
    "    env=mock_env,\n",
    "    agent=pg_agent,\n",
    "    num_episodes=50,  # Short training for demo\n",
    "    max_steps=100,\n",
    "    update_freq=5,\n",
    "    save_path=None  # Skip saving for demo\n",
    ")\n",
    "\n",
    "print(\"âœ“ Training completed!\")\n",
    "\n",
    "# Plot training rewards\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(training_rewards, alpha=0.7)\n",
    "plt.title('Policy Gradient Training Rewards')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Moving average\n",
    "window_size = 10\n",
    "if len(training_rewards) >= window_size:\n",
    "    moving_avg = np.convolve(training_rewards, np.ones(window_size)/window_size, mode='valid')\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(moving_avg, color='red', linewidth=2, label=f'Moving Average ({window_size} episodes)')\n",
    "    plt.title('Policy Gradient Training Rewards - Moving Average')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Average Reward')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "print(f\"Final episode reward: {training_rewards[-1]:.2f}\")\n",
    "print(f\"Average reward (last 10 episodes): {np.mean(training_rewards[-10:]):.2f}\")\n",
    "print(f\"Best episode reward: {max(training_rewards):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761427b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "print(\"\\nðŸ“Š Evaluating trained Policy Gradient Agent...\")\n",
    "\n",
    "evaluation_rewards = evaluate_policy_gradient_agent(\n",
    "    env=mock_env,\n",
    "    agent=pg_agent,\n",
    "    num_episodes=10,\n",
    "    max_steps=100\n",
    ")\n",
    "\n",
    "print(\"âœ“ Evaluation completed!\")\n",
    "\n",
    "# Plot evaluation results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(evaluation_rewards)), evaluation_rewards, alpha=0.7, color='skyblue')\n",
    "plt.axhline(y=np.mean(evaluation_rewards), color='red', linestyle='--',\n",
    "            linewidth=2, label=f'Mean: {np.mean(evaluation_rewards):.2f}')\n",
    "plt.title('Policy Gradient Agent Evaluation Results')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Evaluation Statistics:\")\n",
    "print(f\"  Mean Reward: {np.mean(evaluation_rewards):.2f}\")\n",
    "print(f\"  Std Reward: {np.std(evaluation_rewards):.2f}\")\n",
    "print(f\"  Min Reward: {min(evaluation_rewards):.2f}\")\n",
    "print(f\"  Max Reward: {max(evaluation_rewards):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e244f1",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Importing Agents**: Successfully imported all agent classes from the organized structure\n",
    "2. **DQN Agent**: Initialized DQN agent with Q-networks and replay buffer\n",
    "3. **Policy Gradient Agent**: Implemented REINFORCE algorithm with baseline for variance reduction\n",
    "4. **Deep SARSA Agents**: Showed both FinRL-integrated and paper-based SARSA implementations\n",
    "5. **Training & Evaluation**: Demonstrated training loop and performance evaluation\n",
    "\n",
    "### Key Features of Policy Gradient Agent:\n",
    "- **Direct Policy Learning**: Learns action probabilities directly instead of Q-values\n",
    "- **REINFORCE Algorithm**: Uses Monte Carlo policy gradient with discounted returns\n",
    "- **Baseline Subtraction**: Uses value function approximation to reduce gradient variance\n",
    "- **PyTorch Implementation**: Fully vectorized neural network implementation\n",
    "\n",
    "### Next Steps:\n",
    "- Replace mock environment with real FinRL StockTradingEnv\n",
    "- Load actual financial data (FPT, VN30, etc.)\n",
    "- Tune hyperparameters for better performance\n",
    "- Compare with DQN and SARSA agents\n",
    "- Implement more advanced policy gradient methods (PPO, TRPO)\n",
    "\n",
    "The organized codebase structure makes it easy to experiment with different RL algorithms and compare their performance on financial trading tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
